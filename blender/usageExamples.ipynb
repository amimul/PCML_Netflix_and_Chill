{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of usage of Cross Validation while mixing models\n",
    "Not well implemented yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1a76f00208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.display.max_columns = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cross_validation import KFoldIndexes,CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from als import predictions_ALS\n",
    "from means import user_mean,global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Movie  Rating\n",
       "0    44      1       4\n",
       "1    61      1       3\n",
       "2    67      1       4\n",
       "3    72      1       3\n",
       "4    86      1       5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = helpers.load_csv()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with predictions_ALS function\n",
    "Example of usage of a function - for all function the usage should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index=random.sample(range(1176952),1000000)\n",
    "train_index.sort()\n",
    "test_index=list(set(range(1176952))-set(train_index))\n",
    "test_index.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training=train.loc[train_index]\n",
    "testing=train.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_ALS(training,testing,sc,rank=8,lambda_=0.081, iterations=24, nonnegative=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Blending class\n",
    "To save computational power it computes the predictions for different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blending_test=CrossValidationBlending(train,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blending_test.add_model(predictions_ALS,'als')\n",
    "blending_test.add_model(predictions_ALS,'als2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blending_test.add_model(global_mean,'global_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blending_test.add_params_for_model('als',{'spark_context':sc,'rank':4})\n",
    "blending_test.add_params_for_model('als2',{'spark_context':sc,'rank':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL_MEAN] applying\n",
      "copied pred\n",
      "mean computed\n",
      "after apply\n",
      "[GLOBAL_MEAN] done\n",
      "[GLOBAL_MEAN] applying\n",
      "copied pred\n",
      "mean computed\n",
      "after apply\n",
      "[GLOBAL_MEAN] done\n",
      "[GLOBAL_MEAN] applying\n",
      "copied pred\n",
      "mean computed\n",
      "after apply\n",
      "[GLOBAL_MEAN] done\n",
      "[GLOBAL_MEAN] applying\n",
      "copied pred\n",
      "mean computed\n",
      "after apply\n",
      "[GLOBAL_MEAN] done\n"
     ]
    }
   ],
   "source": [
    "blending_test.add_params_for_model('global_mean',{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0078454959397021"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blending_test.evaluate_blending({'als':1,'als2':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossValidationBlending:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data,k):\n",
    "        ''' Initialization function. It creates self.tests_list, the list of all the test dataframe\n",
    "        \n",
    "        @ params\n",
    "            - data, the input dataframe\n",
    "            - k, the number of splits in the cross validation\n",
    "        '''\n",
    "        \n",
    "        # Initialize static class variables\n",
    "        self.models={} # Dict to store all the model functions\n",
    "        self.params={} # Dict to store the params with which running each model\n",
    "        self.predictions={} # Dict to store the predictions for each model with the given parameters\n",
    "        self.real_values=[] # List to store the real values for each chunk\n",
    "        self.blended_predictions=[]\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.k=k\n",
    "        \n",
    "        # Initialize the k_fold_indexes\n",
    "        k_fold_indexes=KFoldIndexes(k,data.shape[0])\n",
    "        \n",
    "        if k>1:\n",
    "            self.tests_list=self.get_tests_database(data,k_fold_indexes)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def add_model(self,function, name):\n",
    "        self.models[name]=function\n",
    "\n",
    "    def add_params_for_model(self,model_name,params_dict):\n",
    "        if model_name not in self.models:\n",
    "            print('Warning: Adding parameters for a non-existing model')\n",
    "        self.params[model_name]=params_dict\n",
    "        self.compute_predictions(model_name)\n",
    "        \n",
    "    def compute_predictions(self,model):\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            if model_name!=model:\n",
    "                continue\n",
    "            self.real_values=[]\n",
    "            \n",
    "            function=self.models[model_name]\n",
    "            try:\n",
    "                arguments=self.params[model_name]\n",
    "            except:\n",
    "                print('Arguments not available for model',model_name)\n",
    "                \n",
    "            self.predictions[model_name]=[]\n",
    "            for comb in itertools.combinations(range(self.k),self.k-1):\n",
    "                trains=[self.tests_list[x] for x in comb]\n",
    "                train=pd.concat(trains)                \n",
    "                \n",
    "                test_index=[x for x in range(self.k) if x not in comb][0]\n",
    "                test=self.tests_list[test_index]\n",
    "                \n",
    "                self.real_values.append(test.Rating)\n",
    "                self.predictions[model_name].append(function(train,test,**arguments))\n",
    "            \n",
    "    def evaluate_blending(self,blending_dict):        \n",
    "        if len(blending_dict)!=len(self.predictions):\n",
    "            print('Different lenght of blending_dict and predictions')\n",
    "            raise()\n",
    "        \n",
    "        self.blended_predictions=[]\n",
    "        for i in range(self.k):\n",
    "#             prediction=0*self.predictions[0]\n",
    "            \n",
    "            cont=0\n",
    "            for model_name in blending_dict:\n",
    "                if cont==0:\n",
    "                    prediction=blending_dict[model_name]*self.predictions[model_name][i].Rating\n",
    "                    cont+=1\n",
    "                else:\n",
    "                    prediction+=blending_dict[model_name]*self.predictions[model_name][i].Rating\n",
    "                \n",
    "            self.blended_predictions.append(prediction)\n",
    "        \n",
    "        predictions_conc=np.array(pd.concat(self.blended_predictions))\n",
    "        real_values_conc=np.array(pd.concat(self.real_values))\n",
    "        rmse=np.sqrt(sum((predictions_conc-real_values_conc)**2)/predictions_conc.shape[0])\n",
    "        return rmse\n",
    "        \n",
    "    def get_tests_database(self,data,k_fold_indexes):\n",
    "        '''Internal function to get the list of test pandas dataframe'''\n",
    "        tests=[]\n",
    "        for i in k_fold_indexes.indexes:\n",
    "            tests.append(data.loc[i[1]])\n",
    "        return tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of simple Cross Validation on a single model\n",
    "Old part - it will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ALSModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,data,**arg):\n",
    "        self.model = ALS.train(data, **arg)\n",
    "    \n",
    "    def predict(self,data):\n",
    "        data_for_preditions=data.map(lambda x: (x[0], x[1]))\n",
    "        self.predictions = self.model.predictAll(data_for_preditions).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    \n",
    "    def evaluate(self,data):\n",
    "        rates_and_preds = data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(self.predictions)\n",
    "        error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv=CrossValidation(train,4,True,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks = [8]\n",
    "lambdas = [0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09]\n",
    "numIters = [24]\n",
    "nbr_models = len(ranks)*len(lambdas)*len(numIters)\n",
    "\n",
    "bestModel = None\n",
    "bestValidationRmse = float(\"inf\")\n",
    "bestRank = 0\n",
    "bestLambda = -1.0\n",
    "bestNumIter = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "    try:\n",
    "        print(rank,lmbda,numIter)\n",
    "        validationRmse = cv.evaluate(ALSModel(),rank=rank,lambda_=lmbda, iterations=numIter, nonnegative=True)\n",
    "        validationRmse = np.mean(validationRmse)\n",
    "        print(\"Model %i/%i: RMSE (validation) = %f\" %(i+1, nbr_models, validationRmse))\n",
    "        print(\"  Trained with rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter))\n",
    "        print(\"\")\n",
    "        if (validationRmse < bestValidationRmse):\n",
    "#             bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "    except:\n",
    "        print(\"Model %i/%i failed!\" %(i+1, nbr_models))\n",
    "        print(\"  Parameters: rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter))\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "# Evaluate the best model on the training set\n",
    "print(\"The best model was trained with rank = %d and lambda = %.1f, \" % (bestRank, bestLambda) \\\n",
    "  + \"and numIter = %d, and its RMSE on the training set is %f\" % (bestNumIter, bestValidationRmse))\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "# testRmse = computeRMSE(bestModel, test_for_predict_RDD, test_RDD)\n",
    "# print(\"RMSE on the test set: %f\"%(testRmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/sampleSubmission.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test for RDD\n",
    "test_prep = test\n",
    "test_prep['UserID'] = test_prep['Id'].apply(lambda x: int(x.split('_')[0][1:]))\n",
    "test_prep['MovieID'] = test_prep['Id'].apply(lambda x: int(x.split('_')[1][1:]))\n",
    "test_prep['Rating'] = test_prep['Prediction']\n",
    "test_prep = test_prep.drop(['Prediction', 'Id'], axis=1)\n",
    "test_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we transform it using sqlContect\n",
    "test_sql = sqlContext.createDataFrame(test_prep)\n",
    "test_rdd = test_sql.rdd\n",
    "test_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestModel=ALSModel()\n",
    "bestModel.fit(train_rdd,rank=8,lambda_=0.081, iterations=24, nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestModel.predict(test_rdd)\n",
    "predictions=bestModel.predictions\n",
    "# predictions = bestModel.predictAll(test_RDD_Kaggle).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = predictions.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_df['UserID'] = pred_df['_1'].apply(lambda x: x['_1'])\n",
    "pred_df['MovieID'] = pred_df['_1'].apply(lambda x: x['_2'])\n",
    "pred_df['Rating'] = pred_df['_2']\n",
    "pred_df = pred_df.drop(['_1', '_2'], axis=1)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df = pred_df.sort_values(by=['MovieID', 'UserID'])\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df.index = range(len(pred_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Prediction'] = pred_df['Rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.drop(['UserID', 'MovieID', 'Rating'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('pred_pyspark_als.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
