\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% Packages added by Joachim

%drow graph
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{capt-of}
\usepackage{verbatim}

% cancel math expression
\usepackage{cancel}

% math
\usepackage{amsmath}

% math symbols
\usepackage{amssymb}

% url
\usepackage{url}

%subfigure
\usepackage{subcaption}
\usepackage{cleveref}

% multiline tabular cell
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
  
% nice fraction
\usepackage{units}



\begin{document}
\title{PCML CS-433: Recommender System}

\author{
  Team: {\bf Netflix and MaCHILL Learning} \\
  Gael Lederrey, SCIPER 204874, gael.lederrey@epfl.ch \\
  Stefano Savar\`e, SCIPER 260960, stefano.savare@epfl.ch \\
  Joachim Muth, SCIPER 214757, joachim.muth@epfl.ch\\ \\
  \textit{School of Computer and Communication Sciences, EPF Lausanne, Switzerland}
}

\maketitle

%========================
\begin{abstract}
Recommender systems are one of the more dynamic fields in the machine learning world. Almost all
internet companies have to keep track and to analyze the user preferences and the interest about the
more recent and advanced algorithm is spreading fast.
In this paper we analyse  different approaches to this problem using a standard movie recommendation
dataset with only the user ratings informations and the RMSE metric.
According to several studies \cite{BellKore_netflix} \cite{schafer2002meta} the best results are
obtained through a clever blend of several models.
Exploiting this approach we blended 12 different methods (8 direct scoring method and 4 iterative
ones) eventually achieving around 0.977 RMSE on the Kaggle's EPFL ML Recommender System challenge. 
%Collaborative Filtering Recommender Systems for movies collection blending 12 different methods (8 direct scoring method and 4 iterative ones) in order to achieve around 0.977 RMSE on Kaggle's EPFL ML Recommender System challenge. 
\end{abstract}

%========================
\section{Data description}

The data represent ratings from $10'000$ users on $1'000$ movies in an integer scale from 1 to 5. This scale represent the number of \textit{stars} given by the users, 1 being the lowest grade and 5 the best.

The training set used to train our algorithm contains $1'176'952$ ratings which represent around 12\% of possible filled ratings. 
An other $1'176'952$ ratings are hidden from us and must be predicted by our recommender algorithm.


%\begin{figure}[htbp]%------- PICTURE---------
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Variances}
%  \caption{Distribution of variances of ratings per user.}
%  \vspace{-3mm}
%  \label{fig:denoise-fourier}
%\end{figure}
%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Ratings}
%  \vspace{-3mm}
%  \caption{Number of movies rated per user.}
%  \label{fig:denoise-wavelet}
%\end{figure}


\begin{figure}[tbp] %-------------- FIGURE -------------
    \centering
    \hspace{-0.6cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Variances}
        \vspace{-3mm}
  \caption{Distribution of variances of ratings per user. No spammers.}
  \label{variances}
    \end{subfigure}%
    \hspace{0.4cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Ratings}
        \vspace{-3mm}
        \caption{Number of movies rated per user. Good user participation.}
        \label{number_ratings}
    \end{subfigure}
    \caption{Statistical description of data}
\end{figure}


%========================
\section{Data exploration}

\subsection{Search for spammers}

One of the first step before starting learning from data is to ensure that they are real ones, and not produced by bots (spammers). As we know spammers can act in different ways: \textbf{uniform spammer} constantly rates movie in the same way, while \textbf{random spammers} randomly rates movies. In order to check their existence, we analyzed the variances of user ratings: uniform spammer would be put in evidence by null variance, while random spammer will present abnormally high variance. Figure (\ref{variances}) shows the gaussian distribution of the rating variances and ensure the data are free of spammers.

\subsection{Participation of users}

Even free of spammers, data can still contains \textbf{inactive users}, i.e. users which subscribed to a plateform but never use it or never rate movies. If they are in too big number compared with active user, they can disturb learning algorithms. Figure (\ref{number_ratings}) shows histogram of number of movies rated by users and confirm us the good participation of the users.

\subsection{User "moods" (deviation)}

Because of mood/education//habits users having the same appreciation of a movie can rate it differently. Indeed, we show in figure (\ref{mood}) that some users systematically rate lower/higher that others. It's important to take this effect into account in both evaluation of a movie and recommendation for the user and proceed to a normalization of the ratings.

\begin{figure}[htbp] %-------------- FIGURE -------------
  \centering
  \includegraphics[width=\columnwidth]{img/Deviation_mean}
  \caption{Difference of mean rating per user compared with overall mean.}
  \vspace{-3mm}
  \label{mood}
\end{figure}

%========================
\section{Models}

\subsection{Global mean/median (2 models)}

The most simple model is to take all the ratings in the train set and apply the mean or the median value. We return this value as the prediction. This give a baseline from which we can compare further model.

\subsection{User/Movie mean/median (4 models)}

Another simple model is to compute the mean or median value for the users or the movies.

\subsection{Movie mean/median with normalized user moods (2 models)}

The third set of model uses the mean or median value for each movie. We also compute the ``mood'' of the users this way:

\begin{equation}
 d_u = \overline{U} - \overline{u} \quad \forall u\in U
\end{equation}
where $\overline{U} = \frac{1}{\#U} \sum_{u\in U} \overline{u}$ and $\overline{u}$ being the average rating of the user $u$.

Then, we return the prediction of a user $u$ on a movie $m$:
\begin{equation}
 p_{m,u} = \overline{m} + d_u
\end{equation}
where $\overline{m}$ is either the mean or the median of the ratings on the movie $m$.

\subsection{Matrix Factorization using Stochastic Gradient Descent (MF-SGD)}
\label{sec:mf-sgd}
Matrix factorization techniques proved to be one of the most effective strategies to implement 
recommender systems. Given $D$ items, $N$ users and the corresponding rating matrix 
$X \in \mathbb{R}^{D \times N}$, we aim to find two matrixes $W \in \mathbb{R}^{D \times K}$ and $Z
\in \mathbb{R}^{N \times K}$ such that the quantity 

\begin{multline}
\label{eq:functional}
E = \frac{1}{2} \sum_{\substack{d=1,...,D \\n=1,...,N}} 
\bigg( x_{dn} - \big( WZ^T \big)_{dn} \bigg) ^2 + \\ 
\frac{\lambda}{2}\|W\|^2 + \frac{\lambda}{2}\|Z\|^2
\end{multline} 

is minimized. $K$ is a parameter,
corresponding of the number of the \textit{latent factors}; $\lambda$ is a scalar 
that weight the regularization terms.

Different techniques have been deployed to solve this minimization problem. In this Subsection we
will present the Stocastic Gradient Descent method, while in the next one we will explain the
Alternating Least Square optimization. 
The Stochastic Gradient Descent method is a faster variant of the standard gradient descent
optimization. The gradient of the functional \ref{eq:functional} is computed only on a single
element of the summation, randomly chosen. The update process then follows the same rules of the batch
gradient descent.
An almost certain convergence to a local minimum is guaranteed under not restrictive hypothesis.

Using the \texttt{scipy.sparse} matrices library, we implemented the SGD method from scratch.
Through a cross validation process we chosed the best values for the paramester $K$, $\lambda$ and
the number of iterations. The results that we obtained will be presented in Section
\ref{sec:model_bench}.

\subsection{Matrix Factorization using Alternating Least Square (ALS)}
ALS is one of the main alternatives to the SGD to solve the problem \ref{eq:functional}.
It is an iterative algorithm consisting in alternately fixing one of the two matrixes $W$ or $Z$,
while optimizing the problem \ref{eq:functional} with respect to the other matrix.
The optimization problem at each iteration is much easier to solve compared to the one solved by the
SGD. A simple least squares technique can be exploited.

%For speed reasons we decided to use the open source framework Apache Spark to implement this method.
One of the most advanced open source frameworks available to solve this problem is Apache Spark. It
is a cluster computing framework that provides to the programmers an application programming
interface to efficiently execute streaming, machine learning or SQL workloads that require fast
iterative access to datasets.
The Spark package \texttt{mllib} contains several implementations of different machine learning
algorithms, including the ALS for collaborative filtering. Therefore we decided to exploit this
library for the implementation.
Spark also allows to specify the parameters $K$, the number of iterations and $\lambda$. We perform a
cross validation to choose the best parameters, whose results will be discussed in Section 
\ref{sec:model_bench}.

\subsection{PyFM}

%We implemented two version of the SGD methods:
%\begin{itemize}
%\item An implementation from scratch, using \texttt{scipy.sparse} matrices library.
%\item An implementation based on the PyFM Python library, a wrapper of the C++ library libFM
%\cite{rendle:tist2012}, one of
%the most advanced matrix factorization libraries.
%\end{itemize}

PyFM is a python implementation of Factorization Machines. This library is a wrapper of the C++ library libFM \cite{rendle:tist2012}, one of the more advanced matrix factorization libraries. It can be found on Github \cite{pyfm}. The idea behind the algorithm is similar to the MF-SGD \ref{sec:mf-sgd}. 

The usage of this library is really simple. It uses a few parameters:
\begin{itemize}
 \item Number of factors corresponding to the scalar $K$ in the MF-SGD.
 \item Number of iterations
 \item Initial learning rate
\end{itemize}

The number of iterations was fixed. The two other parameters were chosen with the use of a simple cross validation process.

\subsection{Collaborative Filtering}

This algorithm is nothing more than another implementation \textcolor{red}{An other implementation of what?}

%========================
\section{Blending}
\label{sec:blending}

The \textit{Bellkor's Pragmatic Chaos} team, winner of 2009 \textit{Netflix Prize}, explain in its paper that its solution was obtained by blending a hundred of different models. \cite{BellKore_netflix} Without having the same amout of models, we proceed the same to obtain our final solution. We performe a weighted sum that we optimize using \textbf{Sequential Least Squares Programming} (SLSQP) method provided by \texttt{scipy.optimize.minimize} library. Initial weights are set to $\nicefrac{1}{n}$ for each model ($n$ being the number of models). 

\subsection{SLSQP method}
Sequential Least Squares Programming method is a \textbf{Quasi-Newton method}. It differs from Newton method by computing the Hessian matrix but by estimating it by successive gradient vector analyze \cite{wiki:quasi_newton}. It's

%========================
\section{Result}

In order to create our recommender algorithm, 2 steps are required. 
\begin{enumerate}
\item Find the best parameters for each models. A cross validation process was performed on the models MF-SGD, ALS, Collab. Filtering and PyFM. %The first step is to find the best parameters for each models. Therefore, a cross validation process was used on the models MF-SGD, ALS, Collab. Filtering and PyFM. 
\item Apply the blending between all the models with another cross validation process.%Once the best parameters are found, the blending is applied between all the models with another cross validation process. 
\end{enumerate}
In this section, we first present the blending of the models as well as their parameters found after cross validation. Then, we present the benchmarks of the models and the blending.

\subsection{Blending}

Table \ref{blending} provides the weights after minimizing on the average RMSE. It also provides the parameters used for each model.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l l l |}
\hline
Model & weight & parameters\\
\hline 
\hline
Global mean			& 2.87634	& - \\
Global median 			& 0.88256	& - \\
User mean 			& -3.81181	& - \\
User median			& 0.00362	& - \\
Movie mean			& -1.57271	& - \\
Movie mean (mood norm.)		& 1.65276	& - \\
Movie median			& -2.27553	& - \\
Movie median (mood norm.)	& 2.27933	& - \\
MF-SGD (mood normalization)	& -0.16857	& \specialcell[t]{$\lambda = 0.004$ \\
							features = 20 \\
							iterations = 20} \\
ALS				& 0.75256	& \specialcell[t]{$\lambda = 0.081$ \\
							rank = 8 \\
							iterations = 24} \\
Collab. Filtering		& 0.04356	& \specialcell[t]{$\alpha = 19$ \\
							features = 20} \\
PyFM				& 0.30050 	& \specialcell[t]{factors = 20\\
							iterations = 100 \\
							learning rate = 0.001 \\} \\
\hline

\end{tabular}
  \caption{Blending of models.}
  \label{blending}
\end{table}

\subsection{Benchmark}
\label{sec:model_bench}

Table \ref{benchmark} presents the average RMSE of each model applied on the validation sets for the blending cross validation process. Last line is the result of the blending on the same validation sets.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l r |}
\hline
Model & RMSE \\
\hline 
\hline
Global mean			& 1.11906 \\
Global median			& 1.12812 \\
User mean			& 1.09531 \\
User median			& 1.15200 \\
Movie mean			& 1.03050 \\
Movie mean (mood norm.)		& 0.99659 \\
Movie median			& 1.09957 \\
Movie median (mood norm.)	& 1.05784 \\
MF-SGD (mood normalization)	& 0.99994 \\
ALS				& 0.98875 \\
Collab. Filtering		& 1.02776 \\
PyFM				& 0.99178 \\ \hline\hline
blending			& 0.96191 \\ 
\hline
\end{tabular}
  \caption{Benchmark of models.}
  \label{benchmark}
\end{table}

The models were not tested individually on the Kaggle Public dataset. The blending gives a result of \textcolor{red}{0.97788}. 

%========================
\section{Discussion of the results}

Excluding the trivial models based on the users/movies means/medians, we mainly focused on Matrix
Factorization algorithms, exploiting different techniques to achieve the best factorization
possible. Content-based filtering algorithms are not suited for this problem since the users and the
movies are fully anonymized and without additional informations.

The blending plays an important role in our project. For the large number of studied models,
finding a good blend between them proved to be a complex task. The solution that we present pushes
as much as possible the RMSE, while at the same time introducing a noticeable overfitting of the
data.
In this Section we will discuss about these aspects.

%We can first discuss about the models. The models we used are quite simple. Therefore, using more
%complex algorithms instead of these would help in achieving better results. However with our blending method, we can use as many algorithms as we want. Indeed, since we're using an optimization algorithm, the optimization can decide itself if it should discard a model or not. However there's some risk to overfit the data. And it is happening a little bit. Indeed, the RMSE on the validations sets is a bit smaller than the RMSE on Kaggle. 

\subsubsection{Choice of the models}

\subsubsection{Why keeping model with and without normalization?}

It should be legitimate to ask why are we keeping both normalized and unnormalized model for
median/mean. Looking at the coefficients give a partial answer. As we see in table \ref{blending}, normalized and unnormalized models oppose themselves almost exactly, with a little advantage for normalized model. The effect is that the method is more taken into account for users in the tail of the mood curve than for the central ones. Then, it allows the optimizer to have finer control on the blending.

\subsubsection{Blending choices}
The role of the blending part is extremely important in obtaining the best score possible. The large
number of models available would be pointless without a good blend. Moreover, we mainly consider
simple models, leaving to the blender the task of building a more complex one.

As mentioned in Section \ref{sec:blending} we used the Sequential Least Square Programming method to
optimize the weighted sum of models. Simpler methods, like grid search, are computationally too
expensive and less accurate than the algorithm used. We spent some time in finding the best
optimization method for our purposes.

We decided to also allow negative weight. This choice was made for two reasons:
\begin{itemize}
\item We obtained better results.
\item Negative weights may help in better fitting the distribution of the training dataset. This can
lead to overfitting, as we discuss in the next subsection, but, at the same time, it also produces
better results.
\end{itemize}
We also did not put an contraint on the sum of the weight (i.e. the sum is not fixed  to 1) for the
same reasons.

\subsubsection{Overfitting}
We apply several techniques to reduce as much as possible the overfitting of the models we used. In
particular we used a 4-folds cross-validation both to determine the best parameters for each model, both to
choose the best weights in the blending.
Despite this, the model slightly overfits the data. For the Kaggle submission We obtained a score of \textcolor{red}{ADD
SCORE} in the training dataset and \textcolor{red}{ADD SCORE} in the validation dataset. 
In the cross validation we obtained a score of \textcolor{red}{ADD SCORE}.
There are many possible reasons underlying this behaviour. Probably the blending process that we
used, although the proven accuracy, introcudes too much complexity in the model, thus overfitting
the training database.
Using simpler models would solve this problem, but would worsen the prediction. We therefore decided
to keep the more complex one.







\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
