\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% Packages added by Joachim

%drow graph
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{capt-of}
\usepackage{verbatim}

% cancel math expression
\usepackage{cancel}

% math
\usepackage{amsmath}

% math symbols
\usepackage{amssymb}

% url
\usepackage{url}

%subfigure
\usepackage{subcaption}
\usepackage{cleveref}

% multiline tabular cell
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
  
% nice fraction
\usepackage{units}

\usepackage[T1]{fontenc}
\usepackage{frcursive}
\usepackage{calligra}
\usepackage{pbsi}

\newcommand{\setfont}[2]{{\fontfamily{#1}\selectfont #2}}

\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{eso-pic}% http://ctan.org/pkg/eso-pic
\usepackage{graphicx}% http://ctan.org/pkg/graphicx

\usepackage[export]{adjustbox}% http://ctan.org/pkg/adjustbox



\begin{document}

%\AddToShipoutPictureBG{%
%  \AtPageUpperLeft{\raisebox{-\height}{\includegraphics[width=1.5in]{img/Dori}}}%
%}

\title{PCML CS-433: Recommender System}

\author{
  %Team: {\bsifamily \large ``Just keep swimming !''} {\hspace{0.1cm} \includegraphics[width=0.3in ,valign=t]{img/Dori}} \\ \\
   \hspace{0.8cm} Team: {\includegraphics[width=2.5in ,valign=t, trim=0cm 0 0 3.8cm]{img/Dory.png}} \\

  %Team: {\bf Netflix and MaCHILL Learning} \\
  Gael Lederrey, SCIPER 204874, gael.lederrey@epfl.ch \\
    Joachim Muth, SCIPER 214757, joachim.muth@epfl.ch\\
  Stefano Savar\`e, SCIPER 260960, stefano.savare@epfl.ch \\
 \\ 

  \textit{School of Computer and Communication Sciences, EPF Lausanne, Switzerland}
}

\maketitle

%========================
\begin{abstract}
%Recommender systems are one of the more dynamic fields in the machine learning world. Almost all
%internet companies have to keep track and to analyze the user preferences and the interest about the
%more recent and advanced algorithm is spreading fast.
%In this paper we analyse  different approaches to this problem using a standard movie recommendation
%dataset with only the user ratings informations and the RMSE metric.
%According to several studies \cite{BellKore_netflix} \cite{schafer2002meta} the best results are
%obtained through a clever blend of several models.
%Exploiting this approach we blended 12 different methods (8 direct scoring method and 4 iterative
%ones) eventually achieving around 0.977 RMSE on the Kaggle's EPFL ML Recommender System challenge. 
Active Collaborative Filtering Recommender Systems for movie collection using blending of \textcolor{red}{12 different methods (8 direct scoring methods and 4 iterative ones) in order to achieve around 0.977} RMSE on Kaggle's EPFL ML Recommender System challenge. 
\end{abstract}

%========================
\section{Introduction}

Collaborative filtering is a set of techniques aiming at the creation of recommender systems. Usually, we define three types of collaborative filter: \textbf{active}, \textbf{passive} and \textbf{content-based} (the best one being obviously a mix of the three). In industry, recommenders are mainly used to suggest new item to users based on their taste: movies, music tracks, items to purchase, ...

The objective of the project is to develop a recommender system using \textbf{active collaborative filtering} (i.e. pseudonymised items\footnote{Items anonymized by an ID, i.e. we do not have access neither to the name nor any content of it.} rated by pseudonymised users).

We first go through a general \textbf{data analysis} in order to evaluate the quality of the data (spammers and participation of the users). Then we test different models, starting from a basic mean given a prediction baseline and improving the score with more advanced algorithms.

According to \textit{BellKor's Pragmatic Chaos}\footnote{Winner team of 2009 Netflix Prize} scientific paper \cite{BellKore_netflix} the best recommender is obtained through a clever blend of several models. \textcolor{red}{Exploiting this approach, \textbf{we ultimately blended 12 different methods (8 direct scoring methods and 4 iterative ones)} eventually achieving around 0.977 RMSE.}

The implemented model will be part of the \textbf{Kaggle's EPFL ML Recommender System} challenge in which predictions are rated by their RMSE compared with ground truth values. All external libraries are allowed as long as they are properly documented.

%========================
\section{Data Description}

The data represent ratings from $10'000$ users on $1'000$ movies in an integer scale from 1 to 5. Both of them are pseudonymized by an ID. This scale represents the number of \textit{stars} given by the users, 1 being the lowest grade and 5 the best.

The training set used to train our algorithm contains $1'176'952$ ratings which represent around 12\% of filled ratings. 
Another $1'176'952$ ratings are hidden from us and must be predicted by our recommender algorithm in order to be scored on Kaggle platform.


%\begin{figure}[htbp]%------- PICTURE---------
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Variances}
%  \caption{Distribution of variances of ratings per user.}
%  \vspace{-3mm}
%  \label{fig:denoise-fourier}
%\end{figure}
%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Ratings}
%  \vspace{-3mm}
%  \caption{Number of movies rated per user.}
%  \label{fig:denoise-wavelet}
%\end{figure}


\begin{figure}[tbp] %-------------- FIGURE -------------
    \centering
    \hspace{-0.6cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Variances}
        \vspace{-3mm}
  \caption{Distribution of variances of ratings per user. No spammers.}
  \label{variances}
    \end{subfigure}%
    \hspace{0.4cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Ratings}
        \vspace{-3mm}
        \caption{Number of movies rated per user. Good user participation.}
        \label{number_ratings}
    \end{subfigure}
    \caption{Statistical description of data \textcolor{red}{Refact label axis}}
\end{figure}


%========================
\section{Data Exploration}

\subsection{Search for Spammers}

One of the first step before starting learning from data is to ensure that they are real ones, and not produced by bots (spammers). As we know, spammers can act in different ways: \textbf{uniform spammers} constantly rate movies in the same way, while \textbf{random spammers} randomly rate movies. In order to check their existence, we analyzed the variances of user ratings: uniform spammers would be put in evidence by null variance, while random spammers present abnormally high variance. Figure (\ref{variances}) shows the Gaussian distribution of the rating variances and ensure the data are free of spammers.

\subsection{Participation of Users}

Even free of spammers, data can still contain \textbf{inactive users}, i.e. users who subscribed to a platform but never use it or never rate movies. If they are in too big number compared with active users, they can disturb learning algorithms. Figure (\ref{number_ratings}) shows histograms of number of movies rated by users and confirm us the good participation of the users.

\subsection{User "Moods" (Deviation)}

Because of mood/education//habits users having the same appreciation of a movie can rate it differently. Indeed, we show in the figure (\ref{mood}) that some users systematically rate lower/higher than others. It's important to take this effect into account in both evaluation of a movie and recommendation for the user and proceed to a normalization of the ratings. As shown later, we will keep some algorithms in both their normalized and unnormalized forms (see section \ref{with_without_norm}).

\begin{figure}[htbp] %-------------- FIGURE -------------
  \centering
  \includegraphics[width=\columnwidth]{img/Deviation_mean}
  \caption{Difference of mean ratings per user compared with overall mean.}
  \vspace{-3mm}
  \label{mood}
\end{figure}

%========================
\section{Models}

\subsection{Global mean/median (2 models)}

The simplest model is to take all the ratings in the train set and apply the mean or the median value. We return this value as the prediction. This give a baseline score from which we can compare further models.

\subsection{User/Movie mean/median (4 models)}

Another simple model is to compute the mean or median value for the users or the movies.

\subsection{Movie mean/median with normalized user moods (2 models)}

The third set of model uses the mean or median value for each movie. We also compute the ``mood'' of the users this way:

\begin{equation}
 d_u = \overline{U} - \overline{u} \quad \forall u\in U
\end{equation}
where $\overline{U} = \frac{1}{\#U} \sum_{u\in U} \overline{u}$ and $\overline{u}$ being the average rating of the user $u$.

Then, we return the prediction of a user $u$ on a movie $m$:
\begin{equation}
 p_{m,u} = \overline{m} + d_u
\end{equation}
where $\overline{m}$ is either the mean or the median of the ratings on the movie $m$.

\subsection{Matrix Factorization using Stochastic Gradient Descent (MF-SGD)}
\label{sec:mf-sgd}
Matrix factorization techniques proved to be one of the most effective strategies to implement 
recommender systems. Given $D$ items, $N$ users and the corresponding rating matrix 
$X \in \mathbb{R}^{D \times N}$, we aim to find two matrices $W \in \mathbb{R}^{D \times K}$ and $Z
\in \mathbb{R}^{N \times K}$ such that the quantity 

\begin{multline}
\label{eq:functional}
E = \frac{1}{2} \sum_{\substack{d=1,...,D \\n=1,...,N}} 
\bigg( x_{dn} - \big( WZ^T \big)_{dn} \bigg) ^2 + \\ 
\frac{\lambda}{2}\|W\|^2 + \frac{\lambda}{2}\|Z\|^2
\end{multline} 

is minimized. $K$ is a parameter,
corresponding to the number of the \textit{latent factors}; $\lambda$ is a scalar 
that weight the regularization terms.

Different techniques have been deployed to solve this minimization problem. In this Subsection we
will present the Stochastic Gradient Descent method, while in the next one we explain the
Alternating Least Square optimization. 
The Stochastic Gradient Descent method is a faster variant of the standard gradient descent
optimization. The gradient of the functional \ref{eq:functional} is computed only on a single
element of the summation, randomly chosen. The update process then follows the same rules of the batch
gradient descent.
An almost certain convergence to a local minimum is guaranteed under not restrictive hypothesis.

Using the \texttt{scipy.sparse} matrix library, we implemented the SGD method from scratch.
Through a cross validation process we chose the best values for the parameters $K$, $\lambda$ and
the number of iterations. The results that we obtained will be presented in Section
\ref{sec:model_bench}.

\subsection{Matrix Factorization using Alternating Least Square (ALS)}
ALS is one of the main alternatives to the SGD to solve the problem \ref{eq:functional}.
It is an iterative algorithm consisting in alternately fixing one of the two matrices $W$ or $Z$,
while optimizing the problem \ref{eq:functional} with respect to the other matrix.
The optimization problem at each iteration is much easier to solve compared to the one solved by the
SGD. A simple least squares technique can be exploited.

%For speed reasons we decided to use the open source framework Apache Spark to implement this method.
One of the most advanced open source frameworks available to solve this problem is Apache Spark. It
is a cluster computing framework that provides to the programmers an application programming
interface to efficiently execute streaming, machine learning or SQL workloads that require fast
iterative access to datasets.
The Spark package \texttt{mllib} contains several implementations of different machine learning
algorithms, including the ALS for collaborative filtering. Therefore we decided to exploit this
library for the implementation.
Spark also allows to specify the parameters $K$, the number of iterations and $\lambda$. We perform a
cross validation to choose the best parameters, whose results will be discussed in Section 
\ref{sec:model_bench}.

\subsection{PyFM}

%We implemented two version of the SGD methods:
%\begin{itemize}
%\item An implementation from scratch, using \texttt{scipy.sparse} matrices library.
%\item An implementation based on the PyFM Python library, a wrapper of the C++ library libFM
%\cite{rendle:tist2012}, one of
%the most advanced matrix factorization libraries.
%\end{itemize}

PyFM is a python implementation of Factorization Machines. This library is a wrapper of the C++ library libFM \cite{rendle:tist2012}, one of the most advanced matrix factorization libraries. It can be found on Github \cite{pyfm}. The idea behind the algorithm is similar to the MF-SGD \ref{sec:mf-sgd}. 

The usage of this library is really simple. It uses a few parameters:
\begin{itemize}
 \item Number of factors corresponding to the scalar $K$ in the MF-SGD.
 \item Number of iterations
 \item Initial learning rate
\end{itemize}

The number of iterations was fixed. The two other parameters were chosen with the use of a simple cross validation process.

\subsection{Matrix Factorization using Ridge Regression (MF-RR)}

\textcolor{red}{Small description}

%========================
\section{Blending}
\label{sec:blending}

The \textit{Bellkor's Pragmatic Chaos} team, winner of 2009 \textit{Netflix Prize}, explain in its paper that its solution was obtained by blending a hundred different models. \cite{BellKore_netflix} Without having the same number of models, we proceed the same to obtain our final solution. We perform a weighted sum that we optimize using \textbf{Sequential Least Squares Programming} (SLSQP) method provided by \texttt{scipy.optimize.minimize} library. Initial weights are set to $\nicefrac{1}{n}$ for each model ($n$ being the number of models). \textcolor{red}{Justify the negative weights as being not only "weights" but also the result of an optimization, and then treated as factor that can compensate between themselves.}

\subsection{SLSQP method}
Sequential Least Squares Programming method is a \textbf{Quasi-Newton method}. Unlike Newton method, it does not compute the Hessian matrix but estimates it by successive gradient vector analyze \cite{wiki:quasi_newton} using \textbf{Broyden-Fletcher-Goldfarb-Shanno} algorithm (BFGS). This method allows optimization for function without knowing Hessian matrix, in a short computation time.

%========================
\section{Result}

In order to create our recommender algorithm, 2 steps are required. 
\begin{enumerate}
\item Find the best parameters for each model. A cross validation process was performed on the models MF-SGD, ALS, Collab. Filtering and PyFM. \textcolor{red}{Are the CV not done on all the models ???}
%The first step is to find the best parameters for each models. Therefore, a cross validation process was used on the models MF-SGD, ALS, Collab. Filtering and PyFM. 
\item Apply the blending between all the models with another cross validation process.%Once the best parameters are found, the blending is applied between all the models with another cross validation process. 
\end{enumerate}
In this section, we first present the blending of the models as well as their parameters found after cross validation. Then, we present the benchmarks of the models and the blending.

\subsection{Blending}

Table \ref{blending} provides the weights after minimizing on the average RMSE. It also provides the parameters used for each model.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l l l |}
\hline
Model & weight & parameters\\
\hline 
\hline
Global mean			& 2.87634	& - \\
Global median 			& 0.88256	& - \\
User mean 			& -3.81181	& - \\
User median			& 0.00362	& - \\
Movie mean			& -1.57271	& - \\
Movie mean (mood norm.)		& 1.65276	& - \\
Movie median			& -2.27553	& - \\
Movie median (mood norm.)	& 2.27933	& - \\
MF-SGD (mood normalization)	& -0.16857	& \specialcell[t]{$\lambda = 0.004$ \\
							features = 20 \\
							iterations = 20} \\
ALS				& 0.75256	& \specialcell[t]{$\lambda = 0.081$ \\
							rank = 8 \\
							iterations = 24} \\
Collab. Filtering		& 0.04356	& \specialcell[t]{$\alpha = 19$ \\
							features = 20} \\
PyFM				& 0.30050 	& \specialcell[t]{factors = 20\\
							iterations = 100 \\
							learning rate = 0.001 \\} \\
\hline

\end{tabular}
  \caption{Blending of models.}
  \label{blending}
\end{table}

\subsection{Benchmark}
\label{sec:model_bench}

Table \ref{benchmark} presents the average RMSE of each model applied on the validation sets for the blending cross validation process. Last line is the result of the blending on the same validation sets.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l r |}
\hline
Model & RMSE \\
\hline 
\hline
Global mean			& 1.11906 \\
Global median			& 1.12812 \\
User mean			& 1.09531 \\
User median			& 1.15200 \\
Movie mean			& 1.03050 \\
Movie mean (mood norm.)		& 0.99659 \\
Movie median			& 1.09957 \\
Movie median (mood norm.)	& 1.05784 \\
MF-SGD (mood normalization)	& 0.99994 \\
ALS				& 0.98875 \\
Collab. Filtering		& 1.02776 \\
PyFM				& 0.99178 \\ \hline\hline
blending			& 0.96191 \\ 
\hline
\end{tabular}
  \caption{Benchmark of models.}
  \label{benchmark}
\end{table}

The blending gives a RMSE of \textcolor{red}{0.97788} on Kaggle's platform. 

%========================
\section{Discussion of the results}

Excluding the trivial models based on the user/movie mean/median, we mainly focused on \textbf{Matrix
Factorization algorithms}, exploiting different techniques to achieve the best factorization
possible. Content-based filtering algorithms are not suited for this problem since the users and the
movies are fully anonymized and without additional information.

The blending plays an important role in our project. For the large number of studied models,
finding a good blend between them proved to be a complex task. The solution that we present pushes
as much as possible the RMSE, while at the same time introducing a noticeable overfitting of the
data. \textcolor{red}{Are we sure about that ?}
In this Section we will discuss about these aspects.

%We can first discuss about the models. The models we used are quite simple. Therefore, using more
%complex algorithms instead of these would help in achieving better results. However with our blending method, we can use as many algorithms as we want. Indeed, since we're using an optimization algorithm, the optimization can decide itself if it should discard a model or not. However there's some risk to overfit the data. And it is happening a little bit. Indeed, the RMSE on the validations sets is a bit smaller than the RMSE on Kaggle. 

\subsubsection{Choice of the models}

\textcolor{red}{To be filled (or removed)}

\subsubsection{Why keep models with and without normalization?}
\label{with_without_norm}

It should be legitimate to ask why we are keeping both normalized and unnormalized model for
median/mean. Looking at the coefficients gives a partial answer. As we see in table \ref{blending}, normalized and unnormalized models oppose themselves almost exactly, with a little advantage for normalized models. The effect is that the method is more taken into account for users in the tail of the Gaussian deviation curve (figure \ref{mood}) than for the central ones. Then, it allows the optimizer to have finer control on the blending.

\subsubsection{Blending choices}
The role of the blending part is extremely important in obtaining the best score possible. The large
number of models available would be pointless without a good blend. Moreover, we mainly consider
simple models, leaving to the blender the task of building a more complex one.

As mentioned in Section \ref{sec:blending} we used the Sequential Least Square Programming method to
optimize the weighted sum of models. Simpler methods, like grid search, are computationally too
expensive and less accurate than the algorithm used. We spent some time in finding the best
optimization method for our purposes. \textcolor{red}{review}

We decided to also allow negative weight. This choice was made for two reasons:
\begin{itemize}
\item We obtained better results.
\item Negative weights may help in better fitting the distribution of the training dataset. This can
lead to overfitting, as we discuss in the next subsection, but, at the same time, it also produces
better results.
\end{itemize}
We also did not put any constraint on the sum of the weight (i.e. the sum is not fixed  to 1) for the
same reasons.
\textcolor{red}{Refact explaining the weights are result of an optimization, not a weights search.}

\subsubsection{Overfitting}
We apply several techniques to reduce as much as possible the overfitting of the models we used. In
particular we used a 5-folds cross-validation both to determine the best parameters for each model and to
choose the best weights in the blending.
Despite this, the model slightly overfits the data. For the Kaggle submission We obtained a score of \textcolor{red}{ADD
SCORE} in the training dataset and \textcolor{red}{ADD SCORE} in the validation dataset. 
In the cross validation we obtained a score of \textcolor{red}{ADD SCORE}.
There are many possible reasons underlying this behaviour. Probably the blending process that we
used, although the proven accuracy, introcudes too much complexity in the model, thus overfitting
the training database.
Using simpler models would solve this problem, but would worsen the prediction. We therefore decided
to keep the more complex one.

\textcolor{red}{Good explanation and proof of the overfit. Remove the other mention of overfitting in discussion (model is not not SO badly overfitted)}





\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
