\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% Packages added by Joachim

%drow graph
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{capt-of}
\usepackage{verbatim}

% cancel math expression
\usepackage{cancel}

% math
\usepackage{amsmath}

% math symbols
\usepackage{amssymb}

% url
\usepackage{hyperref}

%subfigure
\usepackage{subcaption}
\usepackage{cleveref}

% multiline tabular cell
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}



\begin{document}
\title{PCML CS-433: Recommender System}

\author{
  Gael Lederrey, SCIPER 204874, gael.lederrey@epfl.ch \\
  Stefano Savar\`e, SCIPER 260960, stefano.savare@epfl.ch \\
  Joachim Muth, SCIPER 214757, joachim.muth@epfl.ch\\ \\
  \textit{School of Computer and Communication Sciences, EPF Lausanne, Switzerland}
}

\maketitle

%========================
\begin{abstract}

\end{abstract}

%========================
\section{Data description}

The data represent ratings from $10'000$ users on $1'000$ movies in an integer scale from 1 to 5. This scale represent the number of \textit{stars} given by the users, 1 being the lowest grade and 5 the best.

The training set used to train our algorithm contains $1'176'952$ ratings which represent around 12\% of possible filled ratings. 
An other $1'176'952$ ratings are hidden from us and must be predicted by our recommender algorithm.


%\begin{figure}[htbp]%------- PICTURE---------
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Variances}
%  \caption{Distribution of variances of ratings per user.}
%  \vspace{-3mm}
%  \label{fig:denoise-fourier}
%\end{figure}
%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.9 \columnwidth]{img/Ratings}
%  \vspace{-3mm}
%  \caption{Number of movies rated per user.}
%  \label{fig:denoise-wavelet}
%\end{figure}


\begin{figure}[tbp] %-------------- FIGURE -------------
    \centering
    \hspace{-0.6cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Variances}
        \vspace{-3mm}
  \caption{Distribution of variances of ratings per user. No spammers.}
  \label{variances}
    \end{subfigure}%
    \hspace{0.4cm}
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[height=1.2in]{img/Ratings}
        \vspace{-3mm}
        \caption{Number of movies rated per user. Good user participation.}
        \label{number_ratings}
    \end{subfigure}
    \caption{Statistical description of data}
\end{figure}


%========================
\section{Data exploration}

\subsection{Search for spammers}

One of the first step before starting learning from data is to ensure that they are real ones, and not produced by bots (spammers). As we know spammers can act in different ways: \textbf{uniform spammer} constantly rates movie in the same way, while \textbf{random spammers} randomly rates movies. In order to check their existence, we analyzed the variances of user ratings: uniform spammer would be put in evidence by null variance, while random spammer will present abnormally high variance. Figure (\ref{variances}) shows the gaussian distribution of the rating variances and ensure the data are free of spammers.

\subsection{Participation of users}

Even free of spammers, data can still contains \textbf{inactive users}, i.e. users which subscribed to a plateform but never use it or never rate movies. If they are in too big number compared with active user, they can disturb learning algorithms. Figure (\ref{number_ratings}) shows histogram of number of movies rated by users and confirm us the good participation of the users.

\subsection{User "moods"}

Because of mood/education//habits users having the same appreciation of a movie can rate it differently. Indeed, we show in figure (\ref{mood}) that some users systematically rate lower/higher that others. It's important to take this effect into account in both evaluation of a movie and recommendation for the user and proceed to a normalization of the ratings.

\begin{figure}[htbp] %-------------- FIGURE -------------
  \centering
  \includegraphics[width=\columnwidth]{img/Deviation_mean}
  \caption{Difference of mean rating per user compared with overall mean.}
  \vspace{-3mm}
  \label{mood}
\end{figure}

%========================
\section{Models}

\subsection{Global mean/median (2 models)}

The most simple model is to take all the ratings in the train set and apply the mean or the median value. We return this value as the prediction. This give a baseline from which we can compare further model.

\subsection{User/Movie mean/median (4 models)}

Another simple model is to compute the mean or median value for the users or the movies.

\subsection{Movie mean/median with User mood (2 models)}

The third set of model uses the mean or median value for each movie. We also compute the ``mood'' of the users this way:

\begin{equation}
 d_u = \overline{U} - \overline{u} \quad \forall u\in U
\end{equation}
where $\overline{U} = \frac{1}{\#U} \sum_{u\in U} \overline{u}$ and $\overline{u}$ being the average rating of the user $u$.

Then, we return the prediction of a user $u$ on a movie $m$:
\begin{equation}
 p_{m,u} = \overline{m} + d_u
\end{equation}
where $\overline{m}$ is either the mean or the median of the ratings on the movie $m$.

\subsection{Matrix Factorization using Stochastic Gradient Descent (MF-SGD)}
\label{sec:mf-sgd}
Matrix factorization techniques proved to be one of the most effective strategies to implement 
recommender systems. Given $D$ items, $N$ users and the corresponding rating matrix 
$X \in \mathbb{R}^{D \times N}$, we aim to find two matrixes $W \in \mathbb{R}^{D \times K}$ and $Z
\in \mathbb{R}^{N \times K}$ such that the quantity 
\begin{equation}
\label{eq:functional}
E = \frac{1}{2} \sum_{\substack{d=1,...,D \\n=1,...,N}} 
\bigg( x_{dn} - \big( WZ^T \big)_{dn} \bigg) ^2 +\frac{\lambda}{2}\|W\|^2 + 
\frac{\lambda}{2}\|Z\|^2
\end{equation} 
is minimized. $K$ is a parameter,
corresponding of the number of the \textit{latent factors}; $\lambda$ is a scalar 
that weight the regularization terms.

Different techniques have been deployed to solve this minimization problem. In this Subsection we
will present the Stocastic Gradient Descent method, while in the next one we will explain the
Alternating Least Square optimization. 
The Stochastic Gradient Descent method is a faster variant of the standard gradient descent
optimization. The gradient of the functional \ref{eq:functional} is computed only on a single
element of the summation, randomly chosen. The update process then follows the same rules of the batch
gradient descent.
An almost certain convergence to a local minimum is guaranteed under not restrictive hypothesis.

Through a cross validation process we chosed the best values for the paramester $K$, $\lambda$ and
the number of iterations. The results that we obtained will be presented in Section
\ref{sec:model_bench}.

\subsection{Matrix Factorization using Alternating Least Square (ALS)}
ALS is one of the main alternatives to the SGD to solve the problem \ref{eq:functional}.
It is an iterative algorithm consisting in alternately fixing one of the two matrixes $W$ or $Z$,
while optimizing the problem \ref{eq:functional} with respect to the other matrix.
The optimization problem at each iteration is much easier to solve compared to the one solved by the
SGD. A simple least squares technique can be exploited.

For speed reasons we decided to use the open source framework Apache Spark to implement this method.
Spark allows to specify the parameters $K$, the number of iterations and $\lambda$. We perform a
cross validation to choose the best parameters, whose results will be discussed in Section 
\ref{sec:model_bench}.

\subsection{PyFM}

We implemented two version of the SGD methods:
\begin{itemize}
\item An implementation from scratch, with the help of the Scipy library to deal with sparse
matrixes.
\item An implementation based on the PyFM Python library, a wrapper of the C++ library libFM
\cite{rendle:tist2012}, one of
the more advanced matrix factorization libraries.
\end{itemize}

PyFM is a python implementation of Factorization Machines. This library is a wrapper of the C++ library libFM \cite{rendle:tist2012}, one of the more advanced matrix factorization libraries. It can be found on Github \cite{pyfm}. The idea behind the algorithm is similar to the MF-SGD \ref{sec:mf-sgd}. 

The usage of this library is really simple. It uses a few parameters:
\begin{itemize}
 \item Number of factors corresponding to the scalar $K$ in the MF-SGD.
 \item Number of iterations
 \item Initial learning rate
\end{itemize}

The number of iterations was fixed. The two other parameters were chosen with the use of a simple cross validation process.

\subsection{Collaborative Filtering}

This algorithm is nothing more than another implementation 

\section{Blending}

The \textit{Bellkor's Pragmatic Chaos} team, winner of 2009 \textit{Netflix Prize} explain in its paper that its solution was obtained by blending a hundred of different models. \cite{BellKore_netflix} Without having the same amout of models, we proceed the same to obtain our final solution. We performe a weighted sum that we optimize using \textbf{Nelder-Mead} method provided by \texttt{scipy.optimize} library. Initial weights are set to $0.1$ for each model.

%========================
\section{Result}

In order to create our recommender algorithm, 2 steps are required. The first step is to find the best parameters for each models. Therefore, a cross validation process was used on the models MF-SGD, ALS, Collab. Filtering and PyFM. Once the best parameters are found, the blending is applied between all the models with another cross validation process. 

In this section, we first present the blending of the models as well as their parameters found after cross validation. Then, we present the benchmarks of the models and the blending.

\subsection{Blending}

Table \ref{blending} provides the weights after minimizing on the average RMSE. It also provides the parameters used for each model.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l l l |}
\hline
Model & weight & parameters\\
\hline 
\hline
Global mean			& 2.87634	& - \\
Global median 			& 0.88256	& - \\
User mean 			& -3.81181	& - \\
User median			& 0.00362	& - \\
Movie mean			& -1.57271	& - \\
Movie mean (mood norm.)		& 1.65276	& - \\
Movie median			& -2.27553	& - \\
Movie median (mood norm.)	& 2.27933	& - \\
MF-SGD (mood normalization)	& -0.16857	& \specialcell[t]{$\lambda = 0.004$ \\
							features = 20 \\
							iterations = 20\\
							initital\_matrix = global\_mean} \\
ALS				& 0.75256	& \specialcell[t]{$\lambda = 0.081$ \\
							rank = 8 \\
							iterations = 24} \\
Collab. Filtering		& 0.04356	& \specialcell[t]{$\alpha = 19$ \\
							features = 20} \\
PyFM				& 0.30050 	& \specialcell[t]{num\_factors = 20\\
							num\_iter = 100 \\
							initial\_learning\_rate = 0.001 \\} \\
\hline

\end{tabular}
  \caption{Blending of models.}
  \label{blending}
\end{table}

\subsection{Benchmark}
\label{sec:model_bench}

Table \ref{benchmark} presents the average RMSE of each model applied on the validation sets for the blending cross validation process. Last line is the result of the blending on the same validation sets.

\begin{table}[htbp]
\centering
\begin{tabular}[c]{| l r |}
\hline
Model & RMSE \\
\hline 
\hline
Global mean			& 1.11906 \\
Global median			& 1.12812 \\
User mean			& 1.09531 \\
User median			& 1.15200 \\
Movie mean			& 1.03050 \\
Movie mean (mood norm.)		& 0.99659 \\
Movie median			& 1.09957 \\
Movie median (mood norm.)	& 1.05784 \\
MF-SGD (mood normalization)	& 0.99994 \\
ALS				& 0.98875 \\
Collab. Filtering		& 1.02776 \\
PyFM				& 0.99178 \\ \hline\hline
blending			& 0.96191 \\ 
\hline
\end{tabular}
  \caption{Benchmark of models.}
  \label{benchmark}
\end{table}

The models were not tested individually on the Kaggle Public dataset. But the blending gives a result of 0.97788. 

%========================
\section{Discussion}

We can first discuss about the models. The models we used are quite simple. Therefore, using more complex algorithms instead of these would help a bit more. 




\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
