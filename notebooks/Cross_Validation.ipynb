{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation of the ALS method using Spark\n",
    "We refer to this tutorial: http://spark.apache.org/docs/latest/ml-tuning.html#example-model-selection-via-cross-validation\n",
    "\n",
    "The tutorial was not clear: we used SKLearn to split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.display.max_columns = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KFoldIndexes:\n",
    "    ''' Class to get indexes for cross validation\n",
    "    \n",
    "    Usage: \n",
    "    indexes=KFoldIndexes(4,10)\n",
    "    indexes.indexes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,n_splits,rows):\n",
    "        test_elements=int(rows/n_splits)\n",
    "        total_elements=list(range(rows))\n",
    "        remaining_elements=rows%n_splits\n",
    "        elements_remaining_list=list(range(rows))\n",
    "        elements_remaining_number=rows\n",
    "        \n",
    "        self.indexes=[]\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            random.shuffle(elements_remaining_list)\n",
    "            test=elements_remaining_list[:test_elements+int(remaining_elements>i)]\n",
    "            train=elements_remaining_list[test_elements+int(remaining_elements>i):]\n",
    "            elements_remaining_list=train.copy()\n",
    "\n",
    "            for j in range(i):\n",
    "                train+=self.indexes[j][1]\n",
    "            \n",
    "            \n",
    "#             test=random.sample(elements_remaining_list, test_elements+int(remaining_elements>i))\n",
    "            \n",
    "#             elements_remaining_number-=len(test_indexes)\n",
    "#             test=[elements_remaining_list[x] for x in test_indexes]\n",
    "#             elements_remaining_list=[x for x in elements_remaining_list if x not in test]\n",
    "#             train=[x for x in total_elements if x not in test]\n",
    "#             print(test_indexes,)\n",
    "\n",
    "            \n",
    "            \n",
    "            self.indexes.append((train,test))\n",
    "        for i in self.indexes:\n",
    "            i[0].sort()\n",
    "            i[1].sort()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/data_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['UserID'] = train['Id'].apply(lambda x: int(x.split('_')[0][1:]))\n",
    "train['MovieID'] = train['Id'].apply(lambda x: int(x.split('_')[1][1:]))\n",
    "train['Rating'] = train['Prediction']\n",
    "train = train.drop(['Id', 'Prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176952"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_folds=4\n",
    "k_fold_indexes=KFoldIndexes(k_folds,train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating\n",
       "1      61        1       3\n",
       "2      67        1       4\n",
       "3      72        1       3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tests_database(df,k_fold_indexes):\n",
    "    tests=[]\n",
    "    for i in k_fold_indexes.indexes:\n",
    "        tests.append(df.loc[i[1]])\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sql_from_pd(df_list):\n",
    "    sql_list=[]\n",
    "    for i in df_list:\n",
    "        print(1)\n",
    "        sql_list.append(sqlContext.createDataFrame(i))\n",
    "    return sql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests=get_tests_database(train,k_fold_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "tests_sql=get_sql_from_pd(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr=tests_sql[0].rdd.union(tests_sql[1].rdd).union(tests_sql[2].rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts=tests_sql[3].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ALS.train(tr, 2, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_for_predict_RDD = ts.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rates_and_preds = ts.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971935506177453"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[UserID: bigint, MovieID: bigint, Rating: bigint],\n",
       " DataFrame[UserID: bigint, MovieID: bigint, Rating: bigint],\n",
       " DataFrame[UserID: bigint, MovieID: bigint, Rating: bigint],\n",
       " DataFrame[UserID: bigint, MovieID: bigint, Rating: bigint]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserID=44, MovieID=1, Rating=4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sql = sqlContext.createDataFrame(train)\n",
    "train_rdd = train_sql.rdd\n",
    "train_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_sql.map(lambda x: (x[0],x[1],float(x[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql2 = train_sql.withColumn(\"Rating\", train_sql[\"Rating\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sql2=sqlContext.createDataFrame(train_sql.map(lambda x: (x[0],x[1],float(x[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|UserID|MovieID|Rating|\n",
      "+------+-------+------+\n",
      "|    44|      1|   4.0|\n",
      "|    61|      1|   3.0|\n",
      "|    67|      1|   4.0|\n",
      "|    72|      1|   3.0|\n",
      "|    86|      1|   5.0|\n",
      "|    90|      1|   4.0|\n",
      "|   108|      1|   3.0|\n",
      "|   114|      1|   3.0|\n",
      "|   120|      1|   2.0|\n",
      "|   135|      1|   5.0|\n",
      "|   152|      1|   4.0|\n",
      "|   165|      1|   3.0|\n",
      "|   182|      1|   3.0|\n",
      "|   310|      1|   3.0|\n",
      "|   318|      1|   1.0|\n",
      "|   333|      1|   3.0|\n",
      "|   355|      1|   2.0|\n",
      "|   390|      1|   4.0|\n",
      "|   401|      1|   4.0|\n",
      "|   410|      1|   2.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sql2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'_3'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sql2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "\n",
    "als=ALS(userCol='UserID',itemCol='MovieID',)\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter,[10,20])\\\n",
    "    .build()\n",
    "#     .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .build()\n",
    "    \n",
    "# metrics = RegressionMetrics()\n",
    "\n",
    "# Squared Error\n",
    "# print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "# print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\", \n",
    "                                                        labelCol=\"Rating\", predictionCol=\"Rating\" ),\n",
    "                          numFolds=10)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvModel=crossval.fit(train_sql2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.CrossValidatorModel"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql[train_sql.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(train_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(train_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(a.indexes[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3fae54059a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sql\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MovieID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "train_dan=train_sql[col('MovieID').isin(a.indexes[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dan=train_sql[col('MovieID').isin(a.indexes[0][1])]\n",
    "test_for_prediction=test_dan.rdd.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(train_dan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ALS.fit(train_dan, 6, 20, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions=model.predictAll(test_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dan.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dan.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dan.count()+test_dan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_for_prediction.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rates_and_preds = test_dan.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "print(rates_and_preds)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation=train_sql.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predictAll(validation).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sql[col('UserID').isin([1])].take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_mask_from_array(array,max_len):\n",
    "    mask=[0]*max_len\n",
    "    for i in array:\n",
    "        mask[i]=1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.sample(range(1, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 4]\n",
      "[5, 4, 3]\n",
      "[0, 2]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "a=KFoldIndexes(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 2, 3, 5, 6, 7, 9], [1, 8, 4]),\n",
       " ([0, 1, 2, 3, 4, 8, 9], [7, 6, 5]),\n",
       " ([1, 2, 4, 5, 6, 7, 8, 9], [0, 3]),\n",
       " ([0, 1, 3, 4, 5, 6, 7, 8], [2, 9])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4,rows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks = [2, 4, 6, 8, 10, 12]\n",
    "lambdas = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "numIters = [5, 10, 20]\n",
    "nbr_models = len(ranks)*len(lambdas)*len(numIters)\n",
    "\n",
    "bestModel = None\n",
    "bestValidationRmse = float(\"inf\")\n",
    "bestRank = 0\n",
    "bestLambda = -1.0\n",
    "bestNumIter = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(database,rank,lmbda,numIter):\n",
    "    results=[]\n",
    "    for train_index,test_index in kf.indexes:\n",
    "#         mask_train=bool_mask_from_array(train_index,actions.shape[0])\n",
    "#         mask_graph_train=gl.SArray(mask_train)\n",
    "#         train=actions[mask_graph_train]\n",
    "        train=database[col('UserID').isin(train_index)]\n",
    "        test=database[col('UserID').isin(test_index)]\n",
    "        test_for_prediction=test.rdd.map(lambda x: (x[0], x[1]))\n",
    "#         mask_test=bool_mask_from_array(test_index,actions.shape[0])\n",
    "#         mask_graph_test=gl.SArray(mask_test)\n",
    "#         test=actions[mask_graph_test]\n",
    "\n",
    "        model = ALS.train(train.rdd, rank, numIter, lmbda)\n",
    "        print(train.rdd.take(5))\n",
    "        predictions=model.predictAll(test_for_prediction).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "#         print(predictions.take(5))\n",
    "        print(test.rdd.take(5))\n",
    "        rates_and_preds = test.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "        print(rates_and_preds)\n",
    "        error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "        \n",
    "        results.append(error)\n",
    "    print(results)\n",
    "    return np.array(results).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation(train_sql, 2, 0.1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "als = ALS()\n",
    "evaluator = MulticlassClassificationEvaluator() # + other params as in Scala    \n",
    "\n",
    "pipeline = Pipeline(stages=[als])\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .build()\n",
    "#     .addGrid(als.regParam, [0.1, 0.01]) \\\n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "#     .build()\n",
    "\n",
    "numFolds=4\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=numFolds)\n",
    "\n",
    "model = crossval.fit(train_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = train_rdd.randomSplit([6, 2, 2], seed=0)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeRMSE(model, data, prediction):\n",
    "    predictions = model.predictAll(data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = prediction.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks = [2, 4, 6, 8, 10, 12]\n",
    "lambdas = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "numIters = [5, 10, 20]\n",
    "nbr_models = len(ranks)*len(lambdas)*len(numIters)\n",
    "\n",
    "bestModel = None\n",
    "bestValidationRmse = float(\"inf\")\n",
    "bestRank = 0\n",
    "bestLambda = -1.0\n",
    "bestNumIter = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "    try:\n",
    "        print(rank,numIter,lmbda)\n",
    "        model = ALS.train(training_RDD, rank, numIter, lmbda)\n",
    "        validationRmse = computeRMSE(model, validation_for_predict_RDD, validation_RDD)\n",
    "        print(\"Model %i/%i: RMSE (validation) = %f\" %(i+1, nbr_models, validationRmse))\n",
    "        print(\"  Trained with rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter))\n",
    "        print(\"\")\n",
    "        if (validationRmse < bestValidationRmse):\n",
    "            bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "    except:\n",
    "        print(\"Model %i/%i failed!\" %(i+1, nbr_models))\n",
    "        print(\"  Parameters: rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter))\n",
    "\n",
    "    i += 1\n",
    "    break\n",
    "    \n",
    "# Evaluate the best model on the training set\n",
    "print(\"The best model was trained with rank = %d and lambda = %.1f, \" % (bestRank, bestLambda) \\\n",
    "  + \"and numIter = %d, and its RMSE on the training set is %f\" % (bestNumIter, bestValidationRmse))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "testRmse = computeRMSE(bestModel, test_for_predict_RDD, test_RDD)\n",
    "print(\"RMSE on the test set: %f\"%(testRmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
